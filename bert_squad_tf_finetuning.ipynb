{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "bert_squad_tf_finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfeddema/Nvidia-RH/blob/main/bert_squad_tf_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZcFtRsZTluK"
      },
      "source": [
        "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" width=60 height=60 align=\"left\"/>\n",
        "<img src=\"https://info.nvidia.com/rs/156-OFN-742/images/Red_Hat_new_BW.jpg\" width=100 height=100 align=\"left\"/>\n",
        "\n",
        "<br><br>\n",
        "\n",
        "# BERT Question Answering Fine-Tuning & Deployment Using Triton Inference Server In OCP Kubernetes Cluster   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIvp7M8fTluL"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to: \n",
        "\n",
        "1. Fine-Tune BERT on SQuAD QA dataset on RedHat OpenShift (OCP) kubernetes cluster \n",
        "2. Optimize the model with Nvidia TensorRT\n",
        "3. Deploy the fine-tuned BERT QA TF and TRT model with Nvidia Triton Inference Server on OCP kubernetes cluster\n",
        "4. Observe inference metrics on Prometheus and Grafana"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYGBiHu6TluM"
      },
      "source": [
        "## 1. Requirements\n",
        "\n",
        "- NVIDIA GPU \n",
        "  - A100 or V100 or T4\n",
        "- OpenShift Platform\n",
        "- AWS S3 or GCP storage bucket \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8hXortSPjJN"
      },
      "source": [
        "## 2. Links\n",
        "\n",
        "**Nvidia NGC resources**\n",
        "\n",
        "\n",
        "* BERT Tensorflow: \n",
        "\n",
        "  https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_tensorflow \n",
        "\n",
        "* BERT pre-trained checkpoint: \n",
        "  \n",
        "  https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_pretraining_amp_lamb \n",
        "\n",
        "* BERT fine-tuned on QA checkpoint: \n",
        "\n",
        "  https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_qa_squad11_amp_384\n",
        "\n",
        "* TensorRT\n",
        "\n",
        "  https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt\n",
        "\n",
        "* Triton Inference Server\n",
        "\n",
        "  https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver\n",
        "\n",
        "\n",
        "\n",
        "**RedHat OpenShift resources**\n",
        "\n",
        "* RedHat OpenShift link\n",
        "\n",
        "  http://openshift.com/\n",
        "\n",
        "* OpenShift operators\n",
        "\n",
        "  https://www.openshift.com/learn/topics/operators\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBP0MAJNTluQ"
      },
      "source": [
        "## 3. BERT Question Answering Task\n",
        "\n",
        "Bidirectional Embedding Representations from Transformers (BERT), is a method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks like Question Answering, Sentiment analysis, Named Entity Recognition etc.  \n",
        "\n",
        "The original paper can be found here: https://arxiv.org/abs/1810.04805.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1rcQBXaiJhEqaUNxaXTulA1Nb4FLhoxbj\">\n",
        "\n",
        "NVIDIA's BERT model is an optimized version of Google's official implementation, leveraging *mixed precision arithmetic* and *Tensor Cores* on A100, V100 and T4 GPUs for faster training times while maintaining target accuracy.\n",
        "\n",
        "\n",
        "**Pre-training vs Fine-tuning**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1YKbZedBjiJZwUUxA4Ru9L2PVhglgc9lV\">\n",
        "\n",
        "A language model like BERT requires pre-training to be able to encode a given type of text into representations that contain the full underlying meaning of sentences. Once the meaning of words in the languge gets encoded into the model, one can fine-tune the model to solve a particular problem like QA.\n",
        "\n",
        "Note that, compared to pre-training, fine-tuning is generally far less computationally demanding and hence, we'll use pre-trained BERT from NGC and fine-tune it to provide answers to questions on particular paragraphs from the SQuAD dataset.\n",
        "\n",
        "\n",
        "Based on the model size, we have the following two default configurations of BERT.\n",
        "\n",
        "| **Model** | **Hidden layers** | **Hidden unit size** | **Attention heads** | **Feedforward filter size** | **Max sequence length** | **Parameters** |\n",
        "|:---------:|:----------:|:----:|:---:|:--------:|:---:|:----:|\n",
        "|BERTBASE |12 encoder| 768| 12|4 x  768|512|110M|\n",
        "|BERTLARGE|24 encoder|1024| 16|4 x 1024|512|330M|\n",
        "\n",
        "We will use large pre-trained models avaialble on NGC (NVIDIA GPU Cluster, https://ngc.nvidia.com).\n",
        "There are many configuration available, in particular we will download and use the following:\n",
        "\n",
        "**bert_tf_large_fp16_384**\n",
        "\n",
        "Which is pre-trained using the Wikipedia and Book corpus datasets as training data. \n",
        "We will fine-tune on the SQuAD 1.1 Dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n2nZOqjXHnf"
      },
      "source": [
        "## 3. Dataset\n",
        "\n",
        "**S**tanford **Qu**estion **A**nswering **D**ataset (SQuAD) comprises of more than 100,000+ pairs of Questions on Wikipedia articles and their respective answers. \n",
        "\n",
        "**Links**: \n",
        "\n",
        "- SQuAD download: https://rajpurkar.github.io/SQuAD-explorer/\n",
        "- SQuAD paper: https://arxiv.org/pdf/1606.05250.pdf\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=157Frt0Z3cYaITldxirJqA6JhS9U2yq-t\">\n",
        "\n",
        "\n",
        "The dataset contains the following files:\n",
        "\n",
        "* train-v1.1.json - Used for fine-tuning the model\n",
        "* dev-v1.1.json - Used for validating the model\n",
        "* evaluate-v1.1.py - Used for testing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEfE6Zb1bpuW"
      },
      "source": [
        "\n",
        "## 4. Fine-tuning BERT\n",
        "\n",
        "Steps:\n",
        "\n",
        "#### 1. Download the training scripts, model checkpoints and dataset from the links in resources section\n",
        "\n",
        "#### 2. Check the parameter in `<bert-training-code>/scripts/run_squad.sh`\n",
        "\n",
        "\n",
        "```\n",
        "batch_size=${1:-\"8\"} : Batch size of the model\n",
        "learning_rate=${2:-\"5e-6\"} : Learning rate \n",
        "precision=${3:-\"fp16\"} : Floating point 16 precision\n",
        "use_xla=${4:-\"true\"} : By default use XLA optimizations\n",
        "num_gpu=${5:-\"8\"} : Number of GPUs to use for training\n",
        "seq_length=${6:-\"384\"} : Maximum input sequence length\n",
        "doc_stride=${7:-\"128\"} : When splitting up a long document into chunks, how much stride to take between chunks\n",
        "bert_model=${8:-\"large\"} : BERT large model\n",
        "```\n",
        "\n",
        "\n",
        " \n",
        "#### 3. We will use the following command to kick off training from run_squad.sh script with the additional arguments:\n",
        "\n",
        "\n",
        "```\n",
        "squad_version: 1.1\n",
        "model_checkpoint: /home/model_checkpoints/model.ckpt-1000000\n",
        "epochs: 1.5\n",
        "```\n",
        "\n",
        "The final command will look like this: \n",
        "\n",
        "\n",
        "\n",
        "`\n",
        "bash scripts/run_squad.sh 1 5e-6 fp16 false 1 384 128 large 1.1 /home/model_checkpoints/model.ckpt-1000000 1.5\n",
        "`\n",
        "\n",
        "#### 4. Now we are ready to package the above steps into a yaml for deployment on OpenShift cluster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5FmZNu5mVTf"
      },
      "source": [
        "## 5. Using Openshift \"oc\" Commands to access the cluster from the CLI and Operator Installation from the Openshift Web Console\n",
        "\n",
        "Users can choose to access the Openshift cluster from either the CLI or the Openshift Web Console.  You can also do a combination of both which we will demostrate here.\n",
        "\n",
        "Display your user name\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pko32y4Jmjca"
      },
      "source": [
        "! oc whoami"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x16qZoO4oVyk"
      },
      "source": [
        "Display the nodes on this Openshift Cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttW_HixSodUe"
      },
      "source": [
        "! oc get nodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUj-xGCWoj6u"
      },
      "source": [
        "Verify that the GPU worker node (with Nvidia T4) is labeled correctly.   Output from the command below will show the Node Feature Discovery (NFD) Operator has labeled this node to indicate is has an Nvidia T4 GPU.  NFD labels the host with node-specific attributes, like PCI cards, kernel or OS version and more. The PCI label from NFD is used to schedule gpu workloads only on nodes that have a gpu special resource (e.g. 0x10DE is the PCI vendor id for NVIDIA).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3Fu1X17ovp-"
      },
      "source": [
        "# copy the first worker node name into next command \n",
        "! oc describe node <worker-node-from-previous-line> | egrep 'Roles|pci'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q1Ft_tGyS2X"
      },
      "source": [
        "Now switch over to your Openshift Web Console. You can get your URL by issuing the command below to get the OCP 4 console route."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ayb1Yg5yZrK"
      },
      "source": [
        "! oc get -n openshift-console route console"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXbeMf2veTgd"
      },
      "source": [
        "## 6. Fine-tuning BERT QA model  Step 1 \n",
        "\n",
        "Download the fine-tuning training scripts, model checkpoints for the BERT model and the Standford Question and Answer dataset (SQuaD) to your local machine. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNI4aMuJu6E7"
      },
      "source": [
        "Download the training scripts for BERT fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPT3ha_1rODB"
      },
      "source": [
        "! wget --content-disposition https://api.ngc.nvidia.com/v2/resources/nvidia/bert_for_tensorflow/versions/20.06.7/zip -O bert_for_tensorflow_20.06.7.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVE_R_XzrpH_"
      },
      "source": [
        "Download the model checkpoints for BERT fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn7EVbklmYxs"
      },
      "source": [
        "! wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/bert_for_tensorflow/versions/1/zip -O bert_for_tensorflow_1.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz1jgzTztWly"
      },
      "source": [
        "Download the SQuaD Data from https://rajpurkar.github.io/SQuAD-explorer/ version 1.1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px-Xkm0rtcWs"
      },
      "source": [
        "! wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json  -O train-v1.1.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9kaS_ydwJJg"
      },
      "source": [
        "! wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json   -O dev-v1.1.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QixSMiVXctdI"
      },
      "source": [
        "! wget https://github.com/allenai/bi-att-flow/archive/master.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9GKIa_A6E2g"
      },
      "source": [
        "! ls -l bert_for_tensorflow_20.06.7.zip bert_for_tensorflow_1.zip train-v1.1.json dev-v1.1.json master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzZ-1LI6TXzG"
      },
      "source": [
        "## Fine-tuning BERT QA model Step 2 \n",
        "Upload the fine-tuning scripts,  BERT model checkpoints and SQuAD data to an S3 bucket.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e-y6dYKFpHE"
      },
      "source": [
        "!aws s3 cp bert_for_tensorflow_20.06.7.zip s3://openshift-bert-demo/bert-finetuning-scripts/bert_for_tensorflow_20.06.7.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmawx8v2nfMf"
      },
      "source": [
        "!aws s3 cp bert_for_tensorflow_1.zip s3://openshift-bert-demo/bert-model-checkpoints/bert_for_tensorflow_1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1cQMVrMn514"
      },
      "source": [
        "!aws s3 cp train-v1.1.json s3://openshift-bert-demo/squad/train-v1.1.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9YzTiI2ofQh"
      },
      "source": [
        "!aws s3 cp dev-v1.1.json s3://openshift-bert-demo/squad/dev-v1.1.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e27UE_BoxFkp"
      },
      "source": [
        "!aws s3 ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R-gK_6y2RVm"
      },
      "source": [
        "## Fine-tuning BERT QA model Step 3\n",
        "Create a persistent volume claim in openshift in the namespace where you plan to run your bert training (fine-tuning) job.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVgmYtXh21Qm"
      },
      "source": [
        "Create a file on your local system containing this yaml. In this example we call the file \"pvc.yaml\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsqtnl372Bcj"
      },
      "source": [
        "\n",
        "\n",
        "```---\n",
        "apiVersion: v1\n",
        "kind: PersistentVolumeClaim\n",
        "metadata:\n",
        "  name: ocs-ml-data\n",
        "spec:\n",
        "  accessModes:\n",
        "  - ReadWriteOnce\n",
        "  resources:\n",
        "    requests:\n",
        "      storage: 200Gi\n",
        "  storageClassName: ocs-storagecluster-cephfs\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhP7QFogNePg"
      },
      "source": [
        "Create the persistent volume claim in OCS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wQKUx3g2MNw"
      },
      "source": [
        "!oc create -f pvc.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GgifnoV4Vbm"
      },
      "source": [
        "!oc get pvc --namespace default"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tMwLxjgM2GO"
      },
      "source": [
        "\n",
        "\n",
        "You can fine-tune BERT to accomplish different tasks.  In this step we are fine-tuning BERT to accomplish a Question/Answer task by presenting an example to the model which is the SQuAD dataset (a labeled dataset). This transfer learning characteric of BERT style models enables it to be adapted for different NLP problems (e.g. sentiment analysis and named entity recognition). \n",
        "\n",
        "Now we will submit the following pod manifest to our Openshift cluster and the Openshift scheduler will schedule our pod on a worker node with an Nvidia GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haU91QGLO_YF"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "apiVersion: v1\n",
        "kind: Pod\n",
        "metadata:\n",
        "  name: bert\n",
        "  namespace: default\n",
        "spec:\n",
        "  restartPolicy: OnFailure\n",
        "  containers:\n",
        "    - name: bert\n",
        "      image: \"nvcr.io/nvidia/tensorflow:20.09-tf1-py3\"\n",
        "      command: [\"/bin/bash\", \"-ec\", \"cd /home/scripts; export BERT_DIR=/home/scripts; export MODEL_DIR=/home/model_checkpoints;\n",
        "      bash scripts/run_squad.sh 1 5e-6 fp16 false 1 384 128 large 1.1 /home/model_checkpoints/model.ckpt-1000000 1.5\"]\n",
        "            env:\n",
        "        - name: NVIDIA_VISIBLE_DEVICES\n",
        "          value: all\n",
        "        - name: NVIDIA_DRIVER_CAPABILITIES\n",
        "          value: \"compute,utility\"\n",
        "        - name: NVIDIA_REQUIRE_CUDA\n",
        "          value: \"cuda>=5.0\"\n",
        "      resources:\n",
        "        limits:\n",
        "          nvidia.com/gpu: 1 # requesting 1 GPU\n",
        "      volumeMounts:\n",
        "      - mountPath: /home\n",
        "        name: ocs-ml-data\n",
        "  volumes:\n",
        "  - name: ocs-ml-data\n",
        "    persistentVolumeClaim:\n",
        "      # directory location on host\n",
        "      claimName: ocs-ml-data \n",
        "      readOnly: false\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXSfVWCixDYX"
      },
      "source": [
        "! cat bert-fine-tuning.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfdiJOKKPzyO"
      },
      "source": [
        "You can fine tune BERT to accomplish different tasks. In our case we are fine-tuning BERT to a accomplish Question/Answer task by presenting an example to the model which is the Stanford Question Answer dataset (SQuAD).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNu5Z5iVzj1R"
      },
      "source": [
        "Submit the fine-tuning job"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2UJoHmvza70"
      },
      "source": [
        "! oc create -f bert-fine-tuning.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxe5gDSKFEeR"
      },
      "source": [
        "View running pods in namespace 'default'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpRf3b_rFOd5"
      },
      "source": [
        "! oc get pods --namespace default"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj4uWLEtFTAz"
      },
      "source": [
        "View logs from fine-tuning (training) pod\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPS8HSv0FcaT"
      },
      "source": [
        "! oc logs -f bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2ynEee5AJnu"
      },
      "source": [
        "## 7. TensorRT optimizations\n",
        "\n",
        "### What is TensorRT?\n",
        "TensorRT is built on CUDA, NVIDIA’s parallel programming model, and enables you to optimize inference for all deep learning frameworks leveraging libraries, development tools and technologies in CUDA-X. \n",
        "\n",
        "TensorRT provides INT8 and FP16 optimizations for production deployments of deep learning inference applications. Reduced precision inference significantly reduces application latency in production.\n",
        "\n",
        "Prominent features of TensorRT:\n",
        "\n",
        "* Weight & Activation Precision Calibration: \n",
        "\n",
        "  Maximizes throughput by quantizing models to INT8 while preserving accuracy\n",
        "\n",
        "\n",
        "* Layer & Tensor Fusion\n",
        "\n",
        "  Optimizes use of GPU memory and bandwidth by fusing nodes in a kernel\n",
        "\n",
        "\n",
        "* Kernel Auto-Tuning\n",
        "\n",
        "  Selects best data layers and algorithms based on target GPU platform\n",
        "  \n",
        "\n",
        "* Dynamic Tensor Memory\n",
        "\n",
        "  Minimizes memory footprint and re-uses memory for tensors efficiently\n",
        "  \n",
        "\n",
        "* Multi-Stream Execution\n",
        "\n",
        "  Scalable design to process multiple input streams in parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWcwmj8_yCbS"
      },
      "source": [
        "Optimizing fine-tuned BERT QA model to TensorRT (TRT)\n",
        "\n",
        "Steps:\n",
        "\n",
        "#### 1. Clone TensorRT Github repository on OCP node: https://github.com/NVIDIA/TensorRT.git\n",
        "\n",
        "#### 2. We'll use TensorRT container from NGC: https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt \n",
        "\n",
        "  This container does not come preinstalled with all the python dependencies. Please install the dependencies by executing the following command from within the container: \n",
        "\n",
        "```\n",
        "/opt/tensorrt/python/python_setup.sh \n",
        "```\n",
        "\n",
        "#### 3. We'll be using /TensorRT/demo/BERT/builder.py script to build our optimized TensorRT engine with the following arguments:\n",
        "\n",
        "\n",
        "```\n",
        "mkdir -p /home/engines && \\                     # Make dir to save model\n",
        "python3 builder.py \\                            # Python script to build TRT engine\n",
        "-m /home/bert-fine-tuned/model.ckpt-8144 \\      # Fine-tuned BERT model\n",
        "-o /home/engines/bert_large_128.engine \\        # Output dir where TRT engine will be stored\n",
        "-b 1 \\                                          # Batch size\n",
        "-s 128 \\                                        # Sequence length\n",
        "--fp32 \\                                        # Precision\n",
        "-c /home/bert-fine-tuned/                       # Config dir\n",
        "\n",
        "```\n",
        "\n",
        "Now we are ready to package the above steps in a yaml for deployment on OCP. We can use the same yaml used for training by modifying the image and command to run in the pod.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "apiVersion: v1\n",
        "kind: Pod\n",
        "metadata:\n",
        "  name: trt\n",
        "  namespace: default\n",
        "spec:\n",
        "  restartPolicy: OnFailure\n",
        "  containers:\n",
        "    - name: trt\n",
        "      image: \"nvcr.io/nvidia/tensorrt:20.09-py3\"\n",
        "      command: [\"/bin/bash\", \"-ec\", \" bash /opt/tensorrt/python/python_setup.sh; cd /home/TensorRT/demo/BERT; mkdir -p /home/engines && python3 builder.py -m /home/bert-fine-tuned/model.ckpt-8144 -o /home/engines/bert_large_128.engine -b 1 -s 128 --fp32 -c /home/bert-fine-tuned/;\"]\n",
        "      env:\n",
        "        - name: NVIDIA_VISIBLE_DEVICES\n",
        "          value: all\n",
        "        - name: NVIDIA_DRIVER_CAPABILITIES\n",
        "          value: \"compute,utility\"\n",
        "        - name: NVIDIA_REQUIRE_CUDA\n",
        "          value: \"cuda>=5.0\"\n",
        "      securityContext:\n",
        "        privileged: true\n",
        "      resources:\n",
        "        limits:\n",
        "          nvidia.com/gpu: 1 # requesting 1 GPU\n",
        "      volumeMounts:\n",
        "      - mountPath: /home\n",
        "        name: ocs-ml-data\n",
        "  volumes:\n",
        "  - name: ocs-ml-data\n",
        "    persistentVolumeClaim:\n",
        "      # directory location on host\n",
        "      claimName: ocs-ml-data\n",
        "      readOnly: false\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7cpirMq6K6q"
      },
      "source": [
        "! oc create -f trt_export.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QEelIOh7qN4"
      },
      "source": [
        "Let's check the status of the pod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQxDIWNg7veI"
      },
      "source": [
        "! oc get pods"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFrzRhnx71Er"
      },
      "source": [
        "Finally, let's check the logs and make sure the engine is created in the output directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY6xUdJj78MX"
      },
      "source": [
        "! oc logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNOtq9Y9AmLT"
      },
      "source": [
        "## 8. Model deployment in Triton Inference Server on OpenShift\n",
        "\n",
        "Once the model is fine-tuned and optimized it can be deployed into production using Triton Inference Server. \n",
        "\n",
        "The Triton Inference Server provides a cloud inferencing solution optimized for NVIDIA GPUs. The server provides an inference service via an HTTP endpoint, allowing remote clients to request inferencing for any AI model at scale. \n",
        "\n",
        "The inference server supports:\n",
        "* All major DL frameworks like TensorFlow, PyTorch, ONNX, TensorRT \n",
        "* Low latency real time inferencing\n",
        "* Dynamic matching to maximize GPU/CPU utilization\n",
        "* GPU Metrics in Prometheus format\n",
        "\n",
        "\n",
        "We'll deploy both BERT QA fine-tuned TensorFlow model and optimized TensorRT model in Triton on OCP\n",
        "\n",
        "Steps:\n",
        "\n",
        "#### 1. Clone Triton Github repository to your local\n",
        " \n",
        "```\n",
        "git clone https://github.com/triton-inference-server/server.git\n",
        "```\n",
        "\n",
        "#### 2. Deployment files are in \\<Triton repo\\>/deploy/single_server\n",
        "\n",
        "\n",
        "```\n",
        "Chart.yaml                    # Helm Chart for deployment\n",
        "dashboard.json                # Grafana dashboard\n",
        "values.yaml                   # kubernetes values file with info on image, model repository etc.\n",
        "templates/deployment.yaml     # Deployment file\n",
        "templates/service.yaml        # Service for exporting metrics \n",
        "```\n",
        "\n",
        "Make sure to include right values in values.yaml \n",
        "\n",
        "\n",
        "```\n",
        "imageName: nvcr.io/nvidia/tritonserver:20.09-py3      # Nvidia Triton server docker image\n",
        "modelRepositoryPath: /home/model_repository           # Model repository path can be link to S3 bucket, GS storage or local folder\n",
        "```\n",
        "\n",
        "Triton requires TensorFlow Models model repository in specific format as shown below:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "<model-repository-path>/ \n",
        "  <model-name>/                                       # bert                            \n",
        "    config.pbtxt\n",
        "    1/                                                # version\n",
        "      model.savedmodel/\n",
        "         <saved-model files>                          # variables\n",
        "```\n",
        "\n",
        "#### 3. Deploy triton server using helm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7XmbAfJU_cm"
      },
      "source": [
        "! helm install triton server/deploy/single_server/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HrUIDGrVMx8"
      },
      "source": [
        "Check if the pod is created and running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjuVFQKSVRSx"
      },
      "source": [
        "! oc get pods"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YktTDOPKVZL6"
      },
      "source": [
        "Check the logs of the pod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLgHzmo-VcF4"
      },
      "source": [
        "! oc logs  # paste pod name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpPTPNi5WFYV"
      },
      "source": [
        "Get external IP of load balancer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYzGSGh5WL9O"
      },
      "source": [
        "! oc get services "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwC--jYKV_4r"
      },
      "source": [
        "Check the status of Triton Inference server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt3RdlWLWYmR"
      },
      "source": [
        "! export triton_ip = \"\"           #paste external IP of load balancer here\n",
        "\n",
        "! curl -v  $triton_ip:8000/v2/health/ready"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}